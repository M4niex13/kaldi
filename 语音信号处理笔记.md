### 服务器配置

   需要在kaldi上运行thchs30语料数据库训练出模型系统，对配置要求较高，时间周期长。现申请一个服务器账号。
	
    1、首先进行GMM-HMM模型、EM训练算法生成语音识别系统；再使用DNN进行特征提取和声学模型、LSTM算法生成识别系统；再使用RNN循环神经网络处理上下文，解码时CTC输出音素串，需词典和语言模型得到识别结果，生成识别系统。
    2、对比不同模型的效果，单音素和三音素的效果对比等，评价词错误率等。
    3、后期将进行timit数据库、aishell2的数据库等的训练模型实验。
   CPU用于解码，内存共用，解码时加载wfst很占内存。  
   GPU，卡多一些做实验快。没有GPU需要设置配置。
   对于thchs30语料库，32G内存，有GPU，100G硬盘空间，就够了。但对于将来还会有更大的数据，大型实验，就不够了。
具体配置如下：

	1、GPUs。英伟达显卡，支持cuda。至少搭载256MB显存。
	2、CPU。4或8核，64G RAM。
	3、200G硬盘。

配置说明：GPU进行前期训练深度神经网络模型。卡多做实验快。CPU负责语音解码阶段，加载WFST占大量内存，数据量大，线程多，内存小了会出现内存泄漏报错。多线程速度快，不易影响识别效果和延时。200G空间将用来安装kaldi和一系列软件、库、thchs30语料数据库、后期的timit、aishell2等语音数据库。   
   cuda6.0以上。
   
### 语料数据库训练时长
   aishell2，1000小时训练2周左右；timit 7.5小时训练几小时；六大方言 200小时，半天；
   
## 语音识别框架 （PPT 2、语音识别）

语料库➡️语音短时信号提取➡️语音识别声学参数➡️语音识别声学模型➡️语音识别语言模型➡️文字

### 语音识别工作流程

把帧识别成状态➡️把状态组合成音素➡️把音素组合成单词

### Mel频率倒谱特征参数(MFCC)

Mel频率倒谱参数利用了听觉原理和倒谱的解相关特性，被证明是在语音识别任务中应用最成功的特征描述之一。

  `Mel(f)= 2595*log10(1+f/700)`
  
### HMM

参考http://www.cnblogs.com/skyme/p/4651331.html

参考https://www.zhihu.com/question/20398418

第一步，构建一个状态网络；
  
第二步，从状态网络中寻找与声音最匹配的路径。
 
把结果限制在预先设定的网络中，类似微积分的以直代曲。

搭建状态网络，是由单词级网络展开成音素网络，再展开成状态网络。语音识别过程其实就是再状态网络中搜索一条最佳路径，语音对应这条路径的概率最大，这称为**解码**。路径搜索的算法是一种动态规划剪枝的算法，称之为Viterbi算法，用于寻找全局最优路径。

### Viterbi algorithm

HMM（隐马尔可夫模型）是用来描述隐含未知参数的统计模型，举一个经典的例子：一个东京的朋友每天根据天气{下雨，天晴}决定当天的活动{公园散步,购物,清理房间}中的一种，我每天只能在twitter上看到她发的推“啊，我前天公园散步、昨天购物、今天清理房间了！”，那么我可以根据她发的推特推断东京这三天的天气。在这个例子里，显状态是活动，隐状态是天气。

任何一个HMM都可以通过下列五元组来描述：

    :param obs:可观测序列状态O
    :param states:隐状态S
    :param start_p:初始概率矩阵（隐状态）π
    :param trans_p:转移概率矩阵（隐状态）A
    :param emit_p: 发射概率 （隐状态表现为显状态的概率）(观测状态转移概率矩阵)B
HMM在实际应用中主要用来解决3类问题。

     1. 评估问题。

   即给定观测序列 O=O1O2O3…Ot和模型参数λ=(A,B,π)，怎样有效计算这一观测序列出现的概率

     2. 解码问题。

   即给定观测序列 O=O1O2O3…Ot和模型参数λ=(A,B,π)，怎样寻找满足这种观察序列意义上最优的隐含状态序列S。（根据几天的出行地点，推出几天的天气序列）

     3. 学习问题。

   即HMM的模型参数λ=(A,B,π)未知，如何求出这3个参数以使观测序列O=O1O2O3…Ot的概率尽可能的大。
   
   
   

#### 状态：理解成比音素更细致的语音单位。通常吧一个音素划分成3个状态。

## 语音识别的基本架构

参考https://www.zhihu.com/question/20398418/answer/167412177

	W* = argmax P(W|Y) 	(1)
	   = argmax P(Y|W)P(W)/P(Y) (贝叶斯公式)(2)
	   = argmax P(Y|W)P(W)	(3)
`W`表示文字序列，`Y`表示语音输入。公式1表示语音识别的目标是在给定语音输入的情况下，找到可能性最大的文字序列。根据Baye’ Rule，可以得到公式2，其中分母表示出现这条语音的概率，它相比于求解的文字序列没有参数关系，可以在求解时忽略，进而得到公式3。公式3中第一部分`P(Y|W)`表示给定一个文字序列出现这条音频的概率，它就是语音识别中的声学模型(Acoustic Model，AM)；第二部分`P(W)`表示出现这个文字序列的概率，它就是语音识别中的语言模型(Language Model, LM)。

## 声学模型(Acoustic Model，AM)

参考https://www.zhihu.com/question/20398418
参考https://www.zhihu.com/question/20398418/answer/167412177

声学模型里面存了一大堆参数，通过这些参数，就可以知道帧和状态对应的概率。获取这一大堆参数的方法叫做“训练”，需要使用巨大数量的语音数据。

若干帧语音对应一个状态，每三个状态组合成一个音素，若干个音素组合成一个单词。只要知道每帧语音对应哪个状态，语音识别的结果也就出来了。那每帧音素对应哪个状态呢，看某帧对应哪个状态的概率最大，那这帧就属于哪个状态。那用到的概率从哪里读取呢？就是从声学模型里读取。

## 语言模型（Language Model， LM）
语言模型的作用可以简单理解为消解多音字的问题，在声学模型给出发音序列之后，从候选的文字序列中找出概率最大的字符串序列。

关于语言模型，目前最常见的是N-Gram语言模型和基于RNN的语言模型

## 解码
传统的语音识别解码都是建立在`WFST`的基础之上，它是将`HMM`、词典以及语言模型编译成一个网络。解码就是在这个`WFST`构造的动态网络空间中，找到最优的输出字符序列。搜索通常使用`Viterbi`算法，另外为了防止搜索空间爆炸，通常会采用剪枝算法，因此搜索得到的结果可能不是最优结果。

### 概率计算
参考https://applenob.github.io/hmm.html#%E5%BD%A2%E5%BC%8F%E5%AE%9A%E4%B9%89
#### 2.1 概率计算问题
联合概率为`P(O,I|λ) = P(O|I,λ)P(I|λ)`
举例，`P(O,I,λ) = P(O|I,λ)P(I|λ)P(λ)`:三者发生的概率 = （从右往左）λ发生的概率 * λ发生下I发生的概率 * λ,I发生下O发生的概率。
所以`P(O,I|λ) = P(O|I,λ)P(I|λ)`就是λ发生下O,I发生下的概率 = λ发生下I发生的概率 * λ,I发生下O发生的概率。









