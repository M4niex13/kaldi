## 知乎live 语音识别的前世今生
## GMM + HMM
应用场景，做输入法，输入一段话，变成文字，智能地往里面加标点符号。
### 特征提取 最常用MFCC

一帧frame信号，通常20-50ms（2-3个周期，人说话频率100Hz），一个音素之内。

**音素**：音标的一个音，比如ei、si等

MFCC维度越高，幅度越小，13维最好。

孤立词识别

对每一帧（有用信息：音色（包络））进行**傅立叶变换**  ➡️**三角滤波**得到filterbank output，基本可以作为特征了➡️**取对数log、离散余弦变换DCT**
压缩成更小的规模，13个采样点表示40个点的信息➡️**MFCC**语音的表示方式。

即一帧信息，转换成一个13维的向量。

MFCC序列是最常用的特征。主要描述频谱包络。
改进：一阶二阶差分，相邻两帧做差，得到上下文关系；各种归一化

怎样计算两个特征序列的距离?
### 动态弯算法 DTW
 - 让待识别语音中的每一帧与模板中最相似的一帧匹配（模板，比如一个yes、no语音，待识别语音去匹配）
 - 但要保持顺序
 - 动态规划算法
 - 总距离为各帧的欧氏距离之和


### GMM (高斯混合模型)
每次说yes可能是不一样的，所以要多录几次yes，如果每个词有多个模板，但总不能每个模板都去和待识别语音比较，最后取一个最好的吧，所以怎么办?
答：把模板压缩成模型。把每个模板切成5段，比如把其中一个模板当成待识别语音，和上下两个模板匹配，匹配的结果是吧某些向量放在同一个阶段，
把每个阶段的向量汇总起来。

五个阶段（五个状态）

简略的表示这些向量：用高斯混合模型来拟合这些向量（13维空间的点）用模型来拟合在高维空间分布的情况，哪些多，哪些少。

GMM循环训练，要训练好多轮，为什么不是一次就达到最优解？：这是一个鸡生蛋蛋生鸡问题，由对齐方式更新模型参数，由模型参数更新对齐方式。首先瞎猜一个对齐方式，第一步得到的模型参数是在这个对齐方式下最优的，但这个对齐方式本身不是很优，所以在这个模型参数下去得到一个更好的对齐方式，然后又是更新的模型参数，循环到收敛。

### 决策树
决策树和HMM怎么耦合的？：决策树主要用在上下文有关模型里，比如five里的i和nine里的i不一样，nine里的i和mine里的i一样，决策，就是进行一些提问，分开不同类，比如问是不是鼻音。

决策树的算法，比如根据最大信息增益来分类结点，一开始所有上下文有关音素全在根结点，问一些问题，把音素分成两批，最后放到叶子结点上。

##### 训练模型
 - 把模板切分成多个段落(怎么切?)
 - 用高斯分布的叠加拟合每段中特征向量的分布。对任一特征向量，可给出概率密度，用概率密度代替向量间的欧式距离。
##### 如何用模型识别未知语音?
 - 用“动态弯”算法对齐待识别语音与模型。
   - 用GMM概率密度代替特征向量间的欧氏距离；
   - 相乘得到 P(待识别语音|模型)（这里隐含了一个独立性假设）
 - 取概率最大的模型为识别结果
 
 ### HMM (隐马尔可夫模型)

### 语言模型
n-gram，比如Bigram（2-gram）、trigram：每个词只与前n-1个词有关。

- Bigram是马尔可夫模型：
  - 下一个词只与当前词有关；**马尔可夫**：在HMM中，也说HMM是马尔可夫模型，是指下一个状态和观测向量只与当前状态有关。
  - 模型是遍历的，不是单向的。
- 可与单词的声学模型复合
  - 得到一门语言的HMM。
 
把语言模型看成一个马尔可夫模型的好处：可以与单词的声学模型复合。 比如PPT上，皮的三个状态，是皮的声学模型，声学模型和语言模型复合后，就得到一个大的复合模型。大的转移概率中，留在自身0.6，转移出去概率0.4，皮到卡是0.5，所以皮到卡是`0.4*0.5 = 0.2`；皮到丘是`0.4*0.3 = 0.12`；

#### 连续语音识别：
对于待识别语音（很多单词），只需要在大的复合模型上走，一边走一边和待识别语音匹配，匹配过程就是计算状态观测概率的过程。最匹配的就是最佳路径。可用viterbi解码解决。有了最佳路径之后，看最佳路径上经过了哪些单词的声学模型，把单词串起来，就是一句话的识别结果。

### 大词汇量语音识别 
当词汇量大时，为每个单词训练HMM不现实。（因为每个单词的模型是对这个单词录很多次音来训练出来的），所以不能以单词为单位训练HMM，而要用更小的单位：音素

两步：音素HMM拼接成单词HMM（用词典拼），单词HMM复合成语言HMM。HCLG：H:HMM C:上下文 L:词典 G:gram语言模型

得到大规模模型后，这个模型怎样进行训练和解码？：
 - 训练
   - 给定许多语音和对应的音素串，求模型参数
   - 每个音素串的HMM是单向的，仍用EM算法。 每句话的音素串已知，所以模型还是单向的，不需要知道语言模型。所以训练过程和之前一样的（EM算法），即先瞎猜一个对齐，比如均匀分割，由它求出模型参数，用模型参数来更新对齐方式，由对齐方式再更新模型参数，这样迭代到收敛。
 - 解码
   - 给定一门语言的HMM（包括语言模型、词典、声学模型）和一条语音，求单词串。
   - （在刚才那个巨大的图中）用Viterbi算法求最佳路径（beam search剪枝（因为路径有太多了））：用状态得到特征向量，得到特征向量最大概率。beam：每条路径和当前最好路径的差别，如果这条路径离最好的概率相差太大，就放弃这条路径，就剪枝。
   - 最佳路径经过的单词为识别结果（也可以得到n-best list或lattice）
   
### 区分式训练   (P32)
 - EM算法是最大似然估计
   – 最大化P(X|W)，W是训练文本 
   – 但P(X|W’)可能更大了，W’是W的竞争者 
   – 导致P(W|X)不一定最大化
 - 区分式训练 (discriminative training)
   – 让P(X|W)大，同时让P(X|W’)小
   – 竞争者从哪儿来?
     - 来自最大似然系统输出的n-best list或lattice
     
比如W是训练的yes，X是语音yes，W‘是训练的no（竞争者），可能出现P(X|W’)比P(X|W)更大的情况，这是不希望发生的，所以要让P(X|W)尽可能大，可以让no的P小，但对于孤立词识别还好，对于连续语音，就没办法设置竞争者了，因为数量太多了。所以用最大似然系统输出n-best，比如hello world，竞争者就是halou world，hello word这种读音接近的语音，让它们的P小。

## 神经网络
神经元:

     y = σ (w1 x1 + w2 x2 + w3 x3 + b)
`σ` 是非线性函数，称为 “激活函数” (activation function) 

一层神经网络，写成向量形式：

    y = σ (Wx + b)
前馈神经网络 (feed-forward neural network)：许多层神经网络摞起来

    y = σ3 (W3 σ2 (W2 σ1 (W1 x + b1 ) + b2 ) + b3 )
多层非线性让网络具有强大的拟合能力，但需要大量训练数据

如何训练神经网络? 
 - 神经网络是一个带参数(W, b)的函数 
 - 设计损失函数 (loss function) （与W、b有关）
 - 梯度下降法 (gradient descent) （损失函数对所有参数的导数）
 - 反向传播 (back-propagation)    （链式法则）
## Tandem结构   （P42）
神经网络不用`MFCC`提取特征了，用`DNN`提取特征。

语音信号➡️特征提取（DNN）➡️解码器（声学模型（GMM+HMM）、词典、语言模型（Bigram））➡️识别结果

 - 用神经网络提取特征
 - 输入:
   - 连续若干帧的滤波器组输出 （滤波器：上文MFCC特征提取需要滤波d）
   – 甚至直接输入波形
 - 输出:
   - 上下文有关音素的分布(多类判别问题)（几千个音素，几千类判别问题）（给它周围的连续若干帧滤波器组输出，或给这一帧以及它周围输入的波形，让神经网络去判决是哪一个上下文有关音素）
   - 标准答案由GMM+HMM系统提供（先让GMM+HMM跑一遍，得到一个还凑合的对齐方式，知道了每一帧所对应的上下文有关音素，用它来作为神经网络的标准答案）
 - 特征来自瓶颈层:醉翁之意不在酒（神经网络训练好了之后，特征从哪儿来：拿中间维数很小的一层出来作为特征（代替MFCC））（后面几层就没用了）
   
## Hybrid结构  （P44）

语音信号➡️解码器（声学模型（DNN+HMM）、词典、语言模型（Bigram））➡️识别结果   （去掉特征提取）

 - 不再进行特征提取
   – 输入为滤波器组输出或波形
 - `DNN+HMM`声学模型
   – 原先，`GMM`提供P(特征|状态)
   – 现在，`DNN`提供P(状态|输入)
 - 成品系统中没有`GMM`
   – 但训练`DNN`时需要`GMM+HMM`系统提供标答
   
   
##   Grapheme系统   (P53)
  词典：每个单词怎么发音，字符和音素之间的对应关系。
  
  语言模型：单词和单词之间怎么连接，哪些单词连接概率大。上下文的任务。
