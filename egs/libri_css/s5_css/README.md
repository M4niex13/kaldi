## How to use the Kaldi-based CSS pipeline with your speech separation output

*Author: Desh Raj (Johns Hopkins University)*
*JSALT 2020*

This is a short tutorial for using the CSS pipeline with your own separated
outputs. The objective is to make it convenient for JSALT researchers working on
speech separation to evaluate their separated audio files in terms of
downstream diarization and speech recognition performance.


### A word about the evaluation metric

The final evaluation metric is concatenated minimum-permutation word error rate, or
cpWER. Briefly, it is computed by "concatenating" speaker-wise utterances in
the reference and hypothesis, and finding the "permutation" between the 
reference and hypothesis speakers that minimizes the WER. For example, if
the reference contains:

> SpkA: Hello, I am a speaker.
>
> SpkB: Nice to meet you.
>
> SpkA: Same here.

And the hypothesis contains:

> Spk1: Hello I am a *seeker*.
>
> Spk2: Nice to meet you.
>
> Spk3: Same here.

Note that the diarizer here incorrectly estimates 3 speakers, when in fact there are
just 2. If we ignore the diarization error, the WER would simply be given as (0+0+1)/11
= 9.1%. But to take diarization into account, we first concatenate the reference
speakers utterances as:

> SpkA: Hello, I am a speaker. Same here.
>
> SpkB: Nice to meet you.

And similarly for the hypothesis (which would be the same as the original since each s
peaker has only 1 utterance). Now the WERs for the possible assignments of reference and 
hypothesis speakers are:

1. A -> 1, B -> 2: 27.3%
2. A -> 1, B -> 3: 81.8%
3. ... and so on

Clearly, assignment 1 is the best, and so the cpWER is given as 27.3%. 

### Why can separation help

Simply put, the baseline diarization model and ASR are both designed to work with a single-speaker
assumption, so they fail miserably in overlapping speech. Separated audio streams help the
diarization module predict correct speaker labels, and the ASR module to correctly transcribe even
the overlapping speech. Both of these help in reducing the cpWER.

### Prerequisites

There are no prerequisites. All we require is that you have the separated audio
streams for all the sessions.

* There is no constraint on the number of streams (although this pipeline has
been tested only on 2 and 3 stream outputs).
* The only requirement is that the audio must be in **wav** format.
* The data preparation scripts need the wav files to be named in a particular
format, but we also provide an example script for this which you can adapt
for your own naming format.
* There is no constraint on the directory structure. All you need to start this
tutorial is your audio stream wav files compressed into a *tar.gz* archive.

### Example for this how-to

Throughout this how-to, I use the 3-stream separated audio files generated by
Hakan Erdogan (Google), which is available at 
[this Google drive link](https://drive.google.com/file/d/1ebHJwrN_sVyX-3f0Z2jrCKOB1WVIdsXK/view?usp=sharing).
At the time of writing this how-to, this pipeline achieves a cpWER of 20.27%/19.28% on
the LibriCSS dev/eval sets, respectively. I will explain how to perform the 
evaluation step-by-step. 

For the purpose of this how-to, the "JSALT server" will refer to the following
address: **JSALT_dev@52.233.70.102**. This is the dev4-k80 node of the JSALT server.
I would suggest using this node so that you can use the pre-compiled Kaldi. To access
the server, please contact Zhuo Chen to obtain the password.

#### Step 1: Setting up Kaldi on the JSALT server

First log in into the JSALT server (dev4-k80) and start a screen to avoid timeouts.

```console
JSALT_dev@dev4-k80:~$ screen -S css-example
```

Create your own directory to perform the decoding and clone the Kaldi `css_baseline`
branch there. **Note that you don't need to compile Kaldi, just clone it.** 

*The home directory seems to have limited storage so I created a jsalt2020 directory
in /mnt which has more storage. But it requires sudo access to work in /mnt. I am
expecting these file system issues to be resolved soon.* 

```console
JSALT_dev@dev4-k80:~$ cd /mnt/jsalt2020
JSALT_dev@dev4-k80:/mnt/jsalt2020$ mkdir example_user && cd example_user
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ git clone --single-branch --branch css_baseline https://github.com/jsalt2020-asrdiar/kaldi.git
```

Now go to the `libri_css` egs directory and change the PATH to point to the
pre-compiled Kaldi location.

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ cd kaldi/egs/libri_css/s5_css
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ vi path.sh
```

In this file, change the first line to:

```console
export KALDI_ROOT=/mnt/jsalt2020/draj/kaldi
```

Also open `cmd.sh` and change all occurences of `queue.pl` with `run.pl`.

#### Step 2: Downloading the pretrained models

For your convenience, I have already put the pretrained models at this location: 
`/mnt/jsalt2020/draj/models.tar.gz`. You just need to extract it in your Kaldi egs directory
as follows:

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ cp /mnt/jsalt2020/draj/models.tar.gz .
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ tar -xvzf models.tar.gz
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ cp -r jsalt_libricss_models/{data,exp} .
```

#### Step 3: Preparing your data in the correct format

Earlier I mentioned that your audio needs to be in the form of wav files,
compressed into a single archive. Navigate to your directory and copy the 
file there. **Don't extract it just yet.**

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ scp <user>@<your-server-ip>:/path/to/tar/gz .
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ ls
kaldi  libricss_separated_3spk_8sec.tgz
```

Note that my example audio streams are saved in `libricss_separated_3spk_8sec.tgz`.

The next step is the single most important step in this tutorial. I mentioned
earlier that the wav files could be in any directory structure, but they must follow
the following naming convention:

> **overlap_ratio_10.0_sil0.1_1.0_session7_actual10.1_channel_1.wav**

It is similar to the naming of the original LibriCSS files, with the addition
of *_channel_1* at the end which denotes the stream. Note: channel here does
not refer to the microphone; it refers to the separated stream.

I understand that you may not have created the streams in this format. You can
modify the following script (which prepares the data for the example audio) to
your requirements.

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ cp /mnt/jsalt2020/draj/wavs/prepare_css.sh
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ chmod +x ./prepare_css.sh
```

Once you have modified the script, you can use it to extract and prepare the
data in the format required. For the example audio, this would be done as

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ ./prepare_css.sh libricss_separated_3spk_8sec.tgz 3
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ ls
kaldi  libricss_separated  libricss_separated_3spk_8sec.tgz  libricss_separated_3stream  prepare_css.sh
```

Make note of the path to your output data directory: **/mnt/jsalt2020/example_user/libricss_separated_3stream**.

#### Step 3: Performing recognition

Now we're ready to run the actual evaluation. Go back to the Kaldi LibriCSS
directory:

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user$ cd kaldi/egs/libri_css/s5_css
```

Change **line 53** to point to the LibriCSS data directory (/home/JSALT_dev/disk/libricss)
and  **line 56** in `run.sh` to point to your prepared wav files directory:

```console
libricss_corpus=/home/JSALT_dev/disk/libricss
wav_files_dir=/mnt/jsalt2020/example_user/libricss_separated_3stream
```

Also reduce the number of jobs since we are running on a single machine. In **line 13**,
change `decode_nj` to 10.

Now save and exit. Activate the `conda` environment for CSS which contains some
required python packages, and then run.

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ conda activate kaldi_css
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ ./run.sh --data-affix _css
```

The `--data-affix` option just adds an affix in your data directory so you can try
out different separated audios without them overwriting each other and messing up
things.

#### Step 4: Viewing results

The whole decoding should take approximately 2-3 hours. At the end, condition-wise scores
will be printed on the screen as follows:

```console
Dev WERs:
best_wer_session0_CH0_0L %WER 10.98 [ 130 / 1184, 34 ins, 12 del, 84 sub ]
best_wer_session0_CH0_0S %WER 15.10 [ 269 / 1782, 67 ins, 23 del, 179 sub ]
best_wer_session0_CH0_OV10 %WER 25.12 [ 465 / 1851, 156 ins, 85 del, 224 sub ]
best_wer_session0_CH0_OV20 %WER 18.86 [ 342 / 1813, 94 ins, 33 del, 215 sub ]
best_wer_session0_CH0_OV30 %WER 20.42 [ 395 / 1934, 117 ins, 40 del, 238 sub ]
best_wer_session0_CH0_OV40 %WER 28.47 [ 636 / 2234, 236 ins, 137 del, 263 sub ]
Eval WERs:
0L %WER 22.36 [ 2446 / 10938, 785 ins, 413 del, 1248 sub ]
0S %WER 19.81 [ 2970 / 14994, 861 ins, 431 del, 1678 sub ]
OV10 %WER 21.39 [ 3412 / 15951, 1060 ins, 580 del, 1772 sub ]
OV20 %WER 23.49 [ 3984 / 16963, 1128 ins, 747 del, 2109 sub ]
OV30 %WER 26.06 [ 4789 / 18376, 1415 ins, 988 del, 2386 sub ]
OV40 %WER 25.45 [ 4818 / 18932, 1410 ins, 676 del, 2732 sub ]
```

These can also be found at the location: `exp/chain_cleaned/tdnn_1d_sp/decode_dev${data_affix}_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer`

To view the average WERs, suppose "_css" is the data affix. Then:

```console
JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ cat 
exp/chain_cleaned/tdnn_1d_sp/decode_dev_css_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer_average
%WER 20.72 [ 2237 / 10798, 704 ins, 330 del, 1203 sub ]

JSALT_dev@dev4-k80:/mnt/jsalt2020/example_user/kaldi/egs/libri_css/s5_css$ cat 
exp/chain_cleaned/tdnn_1d_sp/decode_eval_css_diarized_2stage_rescore/scoring_kaldi_multispeaker/best_wer_average
%WER 23.32 [ 22419 / 96154, 6659 ins, 3835 del, 11925 sub ]
```
